"""
IR, November 2020
Assignment 2: Ranked Retrieval
Autors: Alina Yanchuk, 89093
        Ana Sofia Fernandes, 88739
"""

from indexing.Results import Results
from indexing.WeightedIndexer import WeightedIndexer
from indexing.InvertedSpimi import InvertedSpimi
import csv
from indexing.ImprovedTokenizer import ImprovedTokenizer
from indexing.SimpleTokenizer import SimpleTokenizer
import time
import os
import psutil
import sys

## Class that acts as a pipeline for the all indexing process (calls all the other classes and methods)
class Indexer:

    def __init__(self,tokenizer_type,input_file,weighted_indexer_type):
        self.tokenizer_type=tokenizer_type
        self.input_file=input_file
        self.weighted_indexer_type=weighted_indexer_type

        self.simp = SimpleTokenizer()
        self.improv = ImprovedTokenizer()
        self.inverted_spimi = InvertedSpimi()



    def documents_indexer(self):
        """
        Index the documents and prints/writes the results and relevant information
        
        Follows this pipeline, for each document of Corpus:

                Read document  
                    |
                Tokenize document
                    |             -
                Index tokenized document   :  Inverted Index  -> Weighted Index (uses the Inverted Index structure) 
                    |           
                Store and print results
    
        """

        ## VARIABLES:

        total_docs = 0 # total number of documents
        #total_tokens = 0 # total number of tokens
        indexing_time = 0
        if self.weighted_indexer_type=="-lnc.ltc": self.weighted_indexer_type="-lnc_ltc"
        already_read = [] # Will store the title of documents already read
        real_doc_ids=[] # Will store the real ID of each document   
        doc_ids = {} # Will store the mapping between the real IDs and the generated ones
        

        ## READ EACH DOCUMENT, TOKENIZE IT AND INDEX:

        start_time = time.time()

        with open (self.input_file, mode='r') as csv_to_read:
            csv_reader=csv.DictReader(csv_to_read)
            for row in csv_reader: # Reads, Tokenizes and Index one document at time
                title_abstract = ""
                document_tokens = []
                if row['abstract'] != "":
                    title = row['title'] 
                    if title not in already_read: # Verifies if the document was already read
                        if row['cord_uid'] == "": real_id=row['doi']
                        else: real_id = row['cord_uid']   
                        already_read.append(title) # Add to the list that has the documents that were already read
                        title_abstract = row['title'] + " " + row['abstract'] 
                        if self.tokenizer_type == '-s': # The user chose to use the simpleTokenizer
                            document_tokens = self.simp.simple_tokenizer(title_abstract)
                        else: # The user chose to use the improvedTokenizer
                            document_tokens = self.improv.improved_tokenizer(title_abstract)

                        total_docs += 1 # Will also be used as generated ID for this document 
                        #total_tokens += len(document_tokens) 
                        doc_ids[total_docs] = real_id # Generated ID: real ID

                        self.inverted_spimi.spimi(document_tokens,total_docs) # Supostamente, indexamos cada documento tokenizado aqui ou o crlh xD
        
        self.inverted_spimi.merge_blocks() 
        self.inverted_spimi.final_inverted_index()
        #self.inverted_spimi.show_inverted_index()

        indexing_time=time.time()-start_time


        ## RESULTS:
        
        print("Indexing time: "+str(indexing_time))

        """


        indexer = InvertedIndexer(total_docs) # Inverted Indexer
        for i in range(total_docs):   # Index one document at a time. The id's are auto generated by incrementation, starting at id=1 
            generated_id=i+1 
            indexer.index_document(corpus[i],generated_id)
            doc_ids[generated_id]=real_doc_ids[i]
        indexer.sort_inverted_index() # All documents have been indexed and the final ordered Inverted Indexer created!
        inverted_index=indexer.get_inverted_index()


        weighted_indexer = WeightedIndexer(total_docs, inverted_index ,indexer.get_doc_len(), total_terms)  ## Weighted Indexer
        if(self.weighted_indexer_type=="-bm25"):    # BM25
            weighted_indexer.weighted_index_bm25()
        else:
            weighted_indexer.weighted_index_lnc_ltc()  # LNC.LTC
        weighted_index=weighted_indexer.get_weighted_index()
        


        ## Results:

        results = Results(inverted_index,doc_ids,self.tokenizer_type,self.input_file,weighted_index,self.weighted_indexer_type[1:]) ## Results ( writes informations to files )
        #results.write_inverted_index_to_file()
        results.write_weighted_index_to_file()
        results.write_document_ids_to_file()

        memory_dic = self.format_bytes(weighted_indexer.get_size_in_mem()) # Memory occupied by the structure used


        # Print results:
        if(self.tokenizer_type=="-s"):
            print("\n    Tokenizer used: Simple     Ranking Method: "+self.weighted_indexer_type[1:]+"\n"
                    +"\n--- Indexation time:  %s seconds." % (round(indexing_time,3))
                    +"\n--- Size in memory used by the dictionary structure:  %s %s." % (round(memory_dic[0],3), memory_dic[1])
                    + "\n--- File with the Weighted Index: results/simpleTokenizer/weightedIndex_"+self.weighted_indexer_type[1:]+".txt")

        else:
            print("\n    Tokenizer used: Improved     Ranking Method: "+self.weighted_indexer_type[1:]+"\n"
                    +"\n--- Indexation time:  %s seconds." % (round(indexing_time,3))
                    +"\n--- Size in memory used by the dictionary structure:  %s %s." % (round(memory_dic[0],3), memory_dic[1])
                    + "\n--- File with the Weighted Index: results/improvedTokenizer/weightedIndex_"+self.weighted_indexer_type[1:]+".txt")

        """
        


## AUXILIAR FUNCTIONS:

    def format_bytes(self,size): 
            """
            Makes the conversion of a received size to a human readable one
            """
            power = 2**10 # 2**10 = 1024
            n = 0
            power_labels = {0 : '', 1: 'kilo', 2: 'mega', 3: 'giga', 4: 'tera'}
            while size > power:
                size /= power
                n += 1
            return size, power_labels[n]+'bytes'


