"""
IR, November 2020
Assignment 2: Ranked Retrieval
Autors: Alina Yanchuk, 89093
        Ana Sofia Fernandes, 88739
"""

from indexing.Results import Results
from indexing.WeightedIndexer import WeightedIndexer
from indexing.InvertedSpimi import InvertedSpimi
from indexing.Corpus import CorpusReader
import json
import csv
import time
import os
import psutil
import sys

## Class that acts as a pipeline for the all indexing process (calls all the other classes and methods)
class Indexer:

    def __init__(self,tokenizer_type,input_file,weighted_indexer_type):
        self.tokenizer_type=tokenizer_type
        self.input_file=input_file
        self.weighted_indexer_type=weighted_indexer_type

        self.corpus_reader = CorpusReader(self.input_file)
        if self.tokenizer_type=="s":
            from indexing.SimpleTokenizer import SimpleTokenizer
            self.tokenizer = SimpleTokenizer()
            self.inverted_spimi = InvertedSpimi("models/simpleTokenizer/")
        else:
            from indexing.ImprovedTokenizer import ImprovedTokenizer
            self.tokenizer = ImprovedTokenizer()
            self.inverted_spimi = InvertedSpimi("models/improvedTokenizer/")


    def documents_indexer(self):
        """
        Index the documents and prints/writes the results and relevant information
        
        Follows this pipeline, for each document of Corpus:
                Read document  
                    |
                Tokenize document
                    |             -
                Index tokenized document   :  Inverted Index  -> Weighted Index (uses the Inverted Index structure) 
                    |           
                Store and print results
    
        """

        ## VARIABLES:

        total_docs = 0 # total number of documents
        total_tokens = 0 # total number of tokens
        indexing_time = 0
        if self.weighted_indexer_type=="-lnc.ltc": self.weighted_indexer_type="-lnc_ltc"
        already_read = [] # Will store the title of documents already read
        real_doc_ids=[] # Will store the real ID of each document   
        doc_ids = {} # Will store the mapping between the real IDs and the generated ones
        documents_len = {}

        ## READ EACH DOCUMENT, TOKENIZE IT AND INDEX:

        start_time = time.time()

        while True:
            data = self.corpus_reader.nextChunk()
            if data is None:
                break
            for document in data:  # each document
                title_abstract = ""
                real_id, title, abstract = document[0], document[1], document[2]
                title_abstract = title + abstract
                document_tokens = self.tokenizer.tokenize(title_abstract) # tokenize
                total_docs += 1 # Will also be used as generated ID for this document 
                total_tokens += len(document_tokens) 
                documents_len[total_docs] = len(document_tokens)
                doc_ids[total_docs] = real_id # Generated ID: real ID

                self.inverted_spimi.spimi(document_tokens,total_docs) 
                
        self.inverted_spimi.merge_blocks() 
        #self.inverted_spimi.final_inverted_index()
        #self.inverted_spimi.show_inverted_index()

        #self.inverted_spimi.get_term_positions_dictionary() 
        

        """
        ## SÓ PARA TESTAR/MELHORAR O WEIGHTED INDEX, DEPOIS APAGAR:
        with open("models/documentIDs.txt",'w') as file_ids:
            json.dump(doc_ids, file_ids)

        inverted_index = self.inverted_spimi.final_inverted_index()

        weighted_indexer = WeightedIndexer(total_docs, inverted_index , documents_len, total_tokens)  ## Weighted Indexer
        if(self.weighted_indexer_type=="-bm25"):    # BM25
            weighted_indexer.weighted_index_bm25()
        else:
            weighted_indexer.weighted_index_lnc_ltc()  # LNC.LTC
        weighted_index=weighted_indexer.get_weighted_index()
        self.store_term_positions(weighted_index)
        with open("models/improvedTokenizer/weightedIndex_bm25.txt", 'w') as file_weighted_index:
            for term in weighted_index:
                file_weighted_index.write(term+";"+str(weighted_index[term][0])+";"+json.dumps(weighted_index[term][1])+"\n")
        """
        indexing_time=time.time()-start_time
        
        ##RESULTS:
        
        print("Indexing time: "+str(indexing_time))

        """
        indexer = InvertedIndexer(total_docs) # Inverted Indexer
        for i in range(total_docs):   # Index one document at a time. The id's are auto generated by incrementation, starting at id=1 
            generated_id=i+1 
            indexer.index_document(corpus[i],generated_id)
            doc_ids[generated_id]=real_doc_ids[i]
        indexer.sort_inverted_index() # All documents have been indexed and the final ordered Inverted Indexer created!
        inverted_index=indexer.get_inverted_index()
        
        
        ## Results:
        results = Results(inverted_index,doc_ids,self.tokenizer_type,self.input_file,weighted_index,self.weighted_indexer_type[1:]) ## Results ( writes informations to files )
        #results.write_inverted_index_to_file()
        results.write_weighted_index_to_file()
        results.write_document_ids_to_file()
        memory_dic = self.format_bytes(weighted_indexer.get_size_in_mem()) # Memory occupied by the structure used
        # Print results:
        if(self.tokenizer_type=="-s"):
            print("\n    Tokenizer used: Simple     Ranking Method: "+self.weighted_indexer_type[1:]+"\n"
                    +"\n--- Indexation time:  %s seconds." % (round(indexing_time,3))
                    +"\n--- Size in memory used by the dictionary structure:  %s %s." % (round(memory_dic[0],3), memory_dic[1])
                    + "\n--- File with the Weighted Index: models/simpleTokenizer/weightedIndex_"+self.weighted_indexer_type[1:]+".txt")
        else:
            print("\n    Tokenizer used: Improved     Ranking Method: "+self.weighted_indexer_type[1:]+"\n"
                    +"\n--- Indexation time:  %s seconds." % (round(indexing_time,3))
                    +"\n--- Size in memory used by the dictionary structure:  %s %s." % (round(memory_dic[0],3), memory_dic[1])
                    + "\n--- File with the Weighted Index: models/improvedTokenizer/weightedIndex_"+self.weighted_indexer_type[1:]+".txt")
        """
        
    def store_term_positions(self, weighted_index):
        """
        Will write the resulting index to file using the following format (one term per line):
        term:idf;doc_id:term_weight:pos1,pos2,pos3,…;doc_id:term_weight:pos1,pos2,pos3,…
        """
        term_position_dict = self.inverted_spimi.get_term_positions_dictionary()
        print(term_position_dict)


## AUXILIAR FUNCTIONS:

    def format_bytes(self,size): 
            """
            Makes the conversion of a received size to a human readable one
            """
            power = 2**10 # 2**10 = 1024
            n = 0
            power_labels = {0 : '', 1: 'kilo', 2: 'mega', 3: 'giga', 4: 'tera'}
            while size > power:
                size /= power
                n += 1
            return size, power_labels[n]+'bytes'